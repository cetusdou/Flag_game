# import json
# import os
# import requests
# import re
# from bs4 import BeautifulSoup
# import urllib3
# import time

# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# # --- âš™ï¸ é…ç½® ---
# DATA_DIR = "./data"
# IMG_DIR = "./assets/football_clubs_png"
# JSON_FILE = f"{DATA_DIR}/football_clubs_europe.json"
# HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

# PROXY_PORT = 7890
# PROXIES = {
#     "http": f"http://127.0.0.1:{PROXY_PORT}",
#     "https": f"http://127.0.0.1:{PROXY_PORT}"
# } if PROXY_PORT else None

# os.makedirs(DATA_DIR, exist_ok=True)
# os.makedirs(IMG_DIR, exist_ok=True)

# # ğŸ† èµ›å­£é¡µé¢æº
# SEASONS_URLS = [
#     ("Premier League", "https://en.wikipedia.org/wiki/2024%E2%80%9325_Premier_League"),
#     ("La Liga", "https://en.wikipedia.org/wiki/2024%E2%80%9325_La_Liga"),
#     ("Bundesliga", "https://en.wikipedia.org/wiki/2024%E2%80%9325_Bundesliga"),
#     ("Serie A", "https://en.wikipedia.org/wiki/2024%E2%80%9325_Serie_A"),
#     ("Ligue 1", "https://en.wikipedia.org/wiki/2024%E2%80%9325_Ligue_1"),
#     ("UEFA", "https://en.wikipedia.org/wiki/2024%E2%80%9325_UEFA_Champions_League"),
#     ("Europa", "https://en.wikipedia.org/wiki/2024%E2%80%9325_UEFA_Europa_League") # æ–°å¢æ¬§è”
# ]

# def get_teams_from_season_page(league_name, url):
#     """ (ä¿æŒä¹‹å‰çš„å¢å¼ºç‰ˆé€»è¾‘ï¼Œæ­¤å¤„çœç•¥é‡å¤ä»£ç ï¼Œç›´æ¥ä½¿ç”¨æ ¸å¿ƒé€»è¾‘) """
#     print(f"ğŸ” æ­£åœ¨æ‰«æè”èµ›: {league_name} ...")
#     teams = []
#     try:
#         r = requests.get(url, headers=HEADERS, proxies=PROXIES, verify=False, timeout=20)
#         soup = BeautifulSoup(r.content, 'html.parser')
#         tables = soup.find_all("table", {"class": "wikitable"})
#         valid_tables = []
#         for tb in tables:
#             headers = [th.get_text().strip() for th in tb.find_all("th")]
#             has_team_col = any(h in headers for h in ["Team", "Club", "Teams"])
#             has_feature_col = any(h in headers for h in ["Stadium", "Location", "Venue", "Association", "City", "Ground"])
#             if has_team_col and has_feature_col: valid_tables.append(tb)

#         for target_table in valid_tables:
#             headers = [th.get_text().strip() for th in target_table.find_all("th")]
#             team_col_index = -1
#             for idx, h in enumerate(headers):
#                 if h in ["Team", "Club", "Teams"]:
#                     team_col_index = idx; break
#             if team_col_index == -1: team_col_index = 0

#             for tr in target_table.find_all("tr")[1:]:
#                 cols = tr.find_all(["th", "td"])
#                 if len(cols) <= team_col_index: continue
#                 link = cols[team_col_index].find("a")
#                 if link and "href" in link.attrs:
#                     title = link.get("title")
#                     if not title or "List of" in title or "Stadium" in title: continue
#                     teams.append({"title": title, "league": league_name})
#     except Exception as e:
#         print(f"   âŒ æ‰«æå¤±è´¥: {e}")
#     print(f"   âœ… å‘ç° {len(teams)} æ”¯çƒé˜Ÿ")
#     return teams

# def get_club_details(page_title):
#     """ (ä¿æŒä¹‹å‰çš„å…¨èƒ½æŠ“å–é€»è¾‘) """
#     api_url = "https://en.wikipedia.org/w/api.php"
#     params = { "action": "parse", "page": page_title, "prop": "text", "format": "json", "redirects": 1 }
#     try:
#         r = requests.get(api_url, params=params, headers=HEADERS, proxies=PROXIES, verify=False, timeout=15)
#         data = r.json()
#         if "error" in data: return None, {}
#         soup = BeautifulSoup(data["parse"]["text"]["*"], 'html.parser')
#         infobox = soup.find("table", {"class": "infobox"})
#         if not infobox: return None, {}

#         img_url = None
#         img_tag = infobox.find("img")
#         if img_tag:
#             src = img_tag.get("src")
#             if src:
#                 if src.startswith("//"): src = "https:" + src
#                 if "/thumb/" in src: src = re.sub(r'/\d+px-', '/500px-', src)
#                 img_url = src

#         info = {}
#         for tr in infobox.find_all("tr"):
#             th = tr.find("th"); td = tr.find("td")
#             if th and td:
#                 key = th.get_text().strip().lower()
#                 val = td.get_text(", ", strip=True)
#                 val = re.sub(r'\[.*?\]', '', val)
#                 if "full name" in key: info["full_name"] = val
#                 elif "nickname" in key: info["nickname"] = val
#                 elif "founded" in key: 
#                     yr = re.search(r'\d{4}', val)
#                     info["founded"] = yr.group(0) if yr else val
#                 elif "ground" in key or "stadium" in key: info["ground"] = val
#                 elif "capacity" in key: info["capacity"] = val
#         return img_url, info
#     except: return None, {}

# # --- ğŸ”¥ å¢é‡æ›´æ–°ä¸»ç¨‹åº ğŸ”¥ ---

# print("ğŸš€ ç¬¬ä¸€é˜¶æ®µï¼šè‡ªåŠ¨å‘ç°çƒé˜Ÿåå•...")
# all_teams_map = {}
# for league_name, url in SEASONS_URLS:
#     teams = get_teams_from_season_page(league_name, url)
#     for t in teams:
#         if t['title'] not in all_teams_map:
#             all_teams_map[t['title']] = t

# target_list = list(all_teams_map.values())
# print(f"\nğŸ“‹ æ€»è®¡éœ€å¤„ç†: {len(target_list)} æ”¯çƒé˜Ÿ")

# # 1. è¯»å–ç°æœ‰æ•°æ®
# existing_db = []
# processed_ids = set()

# if os.path.exists(JSON_FILE):
#     with open(JSON_FILE, 'r', encoding='utf-8') as f:
#         try:
#             existing_db = json.load(f)
#             # å»ºç«‹ç´¢å¼•ï¼šå“ªäº› ID å·²ç»æœ‰äº†ï¼Ÿ
#             for item in existing_db:
#                 processed_ids.add(item['id'])
#             print(f"ğŸ“‚ å·²åŠ è½½ç°æœ‰æ•°æ®: {len(existing_db)} æ¡")
#         except:
#             print("âš ï¸ JSONæ–‡ä»¶æŸåæˆ–ä¸ºç©ºï¼Œå°†é‡æ–°å¼€å§‹")

# print("ğŸš€ ç¬¬äºŒé˜¶æ®µï¼šå¢é‡æŠ“å–...")

# count_new = 0
# count_skip = 0

# for item in target_list:
#     title = item['title']
    
#     # ID ç”Ÿæˆé€»è¾‘ (å¿…é¡»ä¸ä¹‹å‰ä¿æŒä¸€è‡´)
#     safe_id = re.sub(r'[^a-zA-Z0-9]', '_', title).lower().strip('_')
#     if len(safe_id) > 25: safe_id = safe_id[:25]
    
#     # ğŸ”¥ æ ¸å¿ƒåˆ¤æ–­ï¼šå¦‚æœ ID å·²å­˜åœ¨ï¼Œç›´æ¥è·³è¿‡
#     if safe_id in processed_ids:
#         # print(f"â© è·³è¿‡: {safe_id}")
#         count_skip += 1
#         continue

#     # å¼€å§‹æŠ“å–æ–°æ•°æ®
#     print(f"[{count_new+1} new] âš½ æŠ“å–: {title[:15]}... ", end="")
    
#     img_url, details = get_club_details(title)
    
#     filename = f"{safe_id}.png"
#     save_path = os.path.join(IMG_DIR, filename)
#     local_ref = f"./assets/football_clubs_png/{filename}"
    
#     entry = {
#         "id": safe_id,
#         "name": title.split(" F.C.")[0].split(" CF")[0],
#         "league": item['league'],
#         "img": local_ref,
#         **details
#     }
    
#     # ä¸‹è½½å›¾ç‰‡
#     if img_url and not os.path.exists(save_path):
#         try:
#             content = requests.get(img_url, headers=HEADERS, proxies=PROXIES, verify=False, timeout=10).content
#             with open(save_path, 'wb') as f: f.write(content)
#             print("ğŸ“¸", end="")
#         except:
#             print("âŒ", end="")
#     else:
#         print("â©", end="")
        
#     print("") # æ¢è¡Œ
    
#     # åŠ å…¥æ•°æ®åº“
#     existing_db.append(entry)
#     processed_ids.add(safe_id) # æ ‡è®°ä¸ºå·²å¤„ç†
#     count_new += 1
    
#     # æ¯ 5 ä¸ªæ–°æ•°æ®ä¿å­˜ä¸€æ¬¡
#     if count_new % 5 == 0:
#         with open(JSON_FILE, 'w', encoding='utf-8') as f:
#             json.dump(existing_db, f, ensure_ascii=False, indent=2)

# # æœ€ç»ˆä¿å­˜
# with open(JSON_FILE, 'w', encoding='utf-8') as f:
#     json.dump(existing_db, f, ensure_ascii=False, indent=2)

# print(f"\nğŸ‰ ä»»åŠ¡ç»“æŸï¼")
# print(f"   â© è·³è¿‡: {count_skip} (å·²å­˜åœ¨)")
# print(f"   âœ… æ–°å¢: {count_new}")
# print(f"   ğŸ“‚ æ€»è®¡: {len(existing_db)} (ä¿å­˜è‡³ {JSON_FILE})")

import json
import os

# --- âš™ï¸ é…ç½® ---
FILE_PATH = "./data/football_clubs_europe.json"

def remove_duplicates():
    if not os.path.exists(FILE_PATH):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {FILE_PATH}")
        return

    print(f"ğŸ“‚ æ­£åœ¨è¯»å–: {FILE_PATH} ...")
    
    with open(FILE_PATH, 'r', encoding='utf-8') as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError:
            print("âŒ JSON æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æ")
            return

    original_count = len(data)
    unique_data = []
    seen_names = set()
    
    # ç»Ÿè®¡é‡å¤æ¥æºï¼ˆå¯é€‰ï¼Œçœ‹çœ‹å“ªäº›è”èµ›é‡å¤äº†ï¼‰
    duplicates_found = []

    for item in data:
        # è·å–åå­—ï¼Œå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œç»Ÿä¸€è½¬å°å†™è¿›è¡Œæ¯”è¾ƒï¼ˆé˜²æ­¢ Arsenal å’Œ arsenal è¢«å½“æˆä¸¤ä¸ªï¼‰
        # ä½†ä¿å­˜æ—¶ä¿ç•™åŸæ ¼å¼
        raw_name = item.get('name', '').strip()
        compare_name = raw_name.lower()
        
        if not compare_name:
            continue

        if compare_name not in seen_names:
            unique_data.append(item)
            seen_names.add(compare_name)
        else:
            # å‘ç°é‡å¤
            duplicates_found.append(f"{raw_name} ({item.get('league', 'æœªçŸ¥')})")

    new_count = len(unique_data)
    removed_count = original_count - new_count

    # --- ä¿å­˜å›åŸæ–‡ä»¶ ---
    with open(FILE_PATH, 'w', encoding='utf-8') as f:
        json.dump(unique_data, f, ensure_ascii=False, indent=2)

    print("-" * 40)
    print(f"ğŸ‰ å»é‡å®Œæˆï¼")
    print(f"ğŸ”´ åŸæœ‰æ•°é‡: {original_count}")
    print(f"ğŸŸ¢ ç°æœ‰æ•°é‡: {new_count}")
    print(f"ğŸ—‘ï¸ ç§»é™¤é‡å¤: {removed_count} æ¡")
    
    if removed_count > 0:
        print("\nğŸ‘‡ è¢«ç§»é™¤çš„é‡å¤é¡¹ç¤ºä¾‹ (å‰5ä¸ª):")
        for d in duplicates_found[:5]:
            print(f"   - {d}")

if __name__ == "__main__":
    remove_duplicates()