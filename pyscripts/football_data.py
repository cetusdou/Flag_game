# import json
# import os
# import requests
# import re
# from bs4 import BeautifulSoup
# import urllib3
# import time

# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# # --- âš™ï¸ é…ç½® ---
# DATA_DIR = "./data"
# IMG_DIR = "./assets/football_clubs_png"
# # ğŸ”¥ å­˜åˆ°ä¸€ä¸ªæ–°æ–‡ä»¶ï¼Œæˆ–è€…è¦†ç›–åŸæ¥çš„
# JSON_FILE = f"{DATA_DIR}/football_clubs_hardcore.json" 
# HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

# PROXY_PORT = 7890
# PROXIES = {
#     "http": f"http://127.0.0.1:{PROXY_PORT}",
#     "https": f"http://127.0.0.1:{PROXY_PORT}"
# } if PROXY_PORT else None

# os.makedirs(DATA_DIR, exist_ok=True)
# os.makedirs(IMG_DIR, exist_ok=True)

# # ğŸ† ç¡¬æ ¸èµ›å­£æº (å…¨çƒè¦†ç›–)
# SEASONS_URLS = [
#     # --- äº”å¤§è”èµ› (Tier 1) ---
#     ("Premier League", "https://en.wikipedia.org/wiki/2024%E2%80%9325_Premier_League"),
#     ("La Liga", "https://en.wikipedia.org/wiki/2024%E2%80%9325_La_Liga"),
#     ("Bundesliga", "https://en.wikipedia.org/wiki/2024%E2%80%9325_Bundesliga"),
#     ("Serie A", "https://en.wikipedia.org/wiki/2024%E2%80%9325_Serie_A"),
#     ("Ligue 1", "https://en.wikipedia.org/wiki/2024%E2%80%9325_Ligue_1"),
    
#     # --- æ¬¡çº§è”èµ› (The Grinder) ---
#     ("EFL Championship", "https://en.wikipedia.org/wiki/2024%E2%80%9325_EFL_Championship"), # è‹±å† 
#     ("2. Bundesliga", "https://en.wikipedia.org/wiki/2024%E2%80%9325_2._Bundesliga"),       # å¾·ä¹™
#     ("Serie B", "https://en.wikipedia.org/wiki/2024%E2%80%9325_Serie_B"),                   # æ„ä¹™
#     ("Segunda DivisiÃ³n", "https://en.wikipedia.org/wiki/2024%E2%80%9325_Segunda_Divisi%C3%B3n"), # è¥¿ä¹™

#     # --- æ¬§æ´²åŠ²æ—… (Tier 2) ---
#     ("Eredivisie", "https://en.wikipedia.org/wiki/2024%E2%80%9325_Eredivisie"),             # è·ç”²
#     ("Primeira Liga", "https://en.wikipedia.org/wiki/2024%E2%80%9325_Primeira_Liga"),       # è‘¡è¶…
#     ("SÃ¼per Lig", "https://en.wikipedia.org/wiki/2024%E2%80%9325_S%C3%BCper_Lig"),          # åœŸè¶…
#     ("Scottish Premiership", "https://en.wikipedia.org/wiki/2024%E2%80%9325_Scottish_Premiership"), # è‹è¶…

#     # --- å—ç¾ (Passion) ---
#     # æ³¨æ„ï¼šå—ç¾èµ›å­£é€šå¸¸æ˜¯è‡ªç„¶å¹´ (2024)
#     # ("BrasileirÃ£o", "https://en.wikipedia.org/wiki/2024_Campeonato_Brasileiro_S%C3%A9rie_A"), # å·´ç”²
#     # ("Argentine Primera", "https://en.wikipedia.org/wiki/2024_Argentine_Primera_Divisi%C3%B3n"), # é˜¿ç”²

#     # --- é‡‘å…ƒ/å…¶ä»– ---
#     ("MLS", "https://en.wikipedia.org/wiki/2024_Major_League_Soccer_season"),               # ç¾èŒè”
#     ("Saudi Pro League", "https://en.wikipedia.org/wiki/2024%E2%80%9325_Saudi_Pro_League"), # æ²™ç‰¹è”
# ]

# def get_teams_from_season_page(league_name, url):
#     """ (é€šç”¨åˆ—è¡¨è§£æé€»è¾‘) """
#     print(f"ğŸ” æ‰«æ: {league_name} ...", end="")
#     teams = []
#     try:
#         r = requests.get(url, headers=HEADERS, proxies=PROXIES, verify=False, timeout=20)
#         soup = BeautifulSoup(r.content, 'html.parser')
#         tables = soup.find_all("table", {"class": "wikitable"})
        
#         valid_tables = []
#         for tb in tables:
#             headers = [th.get_text().strip() for th in tb.find_all("th")]
#             # å…¼å®¹å„ç§è¡¨å¤´å†™æ³•
#             has_team = any(h in headers for h in ["Team", "Club", "Equipe"])
#             has_feat = any(h in headers for h in ["Stadium", "Location", "Venue", "City", "Home city"])
#             if has_team and has_feat: valid_tables.append(tb)

#         if not valid_tables:
#             print(" âš ï¸ æœªæ‰¾åˆ°è¡¨æ ¼")
#             return []

#         # é€šå¸¸ç¬¬ä¸€ä¸ªç¬¦åˆæ¡ä»¶çš„è¡¨æ ¼å°±æ˜¯çƒé˜Ÿåˆ—è¡¨
#         target_table = valid_tables[0]
        
#         # æ‰¾çƒé˜Ÿåå­—åœ¨å“ªä¸€åˆ—
#         headers = [th.get_text().strip() for th in target_table.find_all("th")]
#         col_idx = 0
#         for i, h in enumerate(headers):
#             if h in ["Team", "Club", "Equipe"]: col_idx = i; break
            
#         rows = target_table.find_all("tr")
#         for tr in rows[1:]:
#             cols = tr.find_all(["th", "td"])
#             if len(cols) <= col_idx: continue
            
#             link = cols[col_idx].find("a")
#             if link and "href" in link.attrs:
#                 title = link.get("title")
#                 if not title or "List of" in title or "Stadium" in title: continue
#                 teams.append({"title": title, "league": league_name})
                
#     except Exception as e:
#         print(f" âŒ {e}")
        
#     print(f" âœ… {len(teams)} é˜Ÿ")
#     return teams

# def get_club_details(page_title):
#     """ (æš´åŠ›è§£æ Infobox) """
#     api_url = "https://en.wikipedia.org/w/api.php"
#     params = { "action": "parse", "page": page_title, "prop": "text", "format": "json", "redirects": 1 }
#     try:
#         r = requests.get(api_url, params=params, headers=HEADERS, proxies=PROXIES, verify=False, timeout=15)
#         data = r.json()
#         if "error" in data: return None, {}
#         soup = BeautifulSoup(data["parse"]["text"]["*"], 'html.parser')
#         infobox = soup.find("table", {"class": "infobox"})
#         if not infobox: return None, {}

#         img_url = None
#         img_tag = infobox.find("img")
#         if img_tag:
#             src = img_tag.get("src")
#             if src:
#                 if src.startswith("//"): src = "https:" + src
#                 if "/thumb/" in src: src = re.sub(r'/\d+px-', '/500px-', src)
#                 img_url = src

#         info = {}
#         for tr in infobox.find_all("tr"):
#             th = tr.find("th"); td = tr.find("td")
#             if th and td:
#                 key = th.get_text().strip().lower()
#                 val = td.get_text(", ", strip=True)
#                 val = re.sub(r'\[.*?\]', '', val)
#                 if "full name" in key: info["full_name"] = val
#                 elif "founded" in key: 
#                     yr = re.search(r'\d{4}', val)
#                     info["founded"] = yr.group(0) if yr else val
#                 elif "ground" in key or "stadium" in key: info["ground"] = val
#         return img_url, info
#     except: return None, {}

# # --- ğŸ”¥ ä¸»ç¨‹åº ğŸ”¥ ---
# print("ğŸš€ [ç¡¬æ ¸æ¨¡å¼] å¯åŠ¨å…¨çƒçƒé˜Ÿæ‰«æ...")

# # 1. å‘ç°çƒé˜Ÿ
# all_teams_map = {}
# for league_name, url in SEASONS_URLS:
#     teams = get_teams_from_season_page(league_name, url)
#     for t in teams:
#         # å»é‡ï¼šå¦‚æœçƒé˜Ÿå·²å­˜åœ¨ï¼Œè·³è¿‡ï¼ˆæˆ–è€…ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„è”èµ›ï¼‰
#         if t['title'] not in all_teams_map:
#             all_teams_map[t['title']] = t

# target_list = list(all_teams_map.values())
# print(f"\nğŸ“‹ å»é‡åæ€»ç›®æ ‡: {len(target_list)} æ”¯çƒé˜Ÿ")

# # 2. è¯»å–/åˆå§‹åŒ–æ•°æ®åº“
# existing_db = []
# processed_ids = set()
# if os.path.exists(JSON_FILE):
#     with open(JSON_FILE, 'r', encoding='utf-8') as f:
#         try:
#             existing_db = json.load(f)
#             for item in existing_db: processed_ids.add(item['id'])
#             print(f"ğŸ“‚ è¯»å–ç°æœ‰æ•°æ®: {len(existing_db)} æ¡")
#         except: pass

# # 3. æŠ“å–
# print("ğŸš€ å¼€å§‹æŠ“å–è¯¦ç»†æ•°æ®...")
# count_new = 0
# for i, item in enumerate(target_list):
#     title = item['title']
    
#     # ID ç”Ÿæˆ
#     safe_id = re.sub(r'[^a-zA-Z0-9]', '_', title).lower().strip('_')
#     if len(safe_id) > 25: safe_id = safe_id[:25]
#     if safe_id in processed_ids: continue # è·³è¿‡å·²å­˜åœ¨

#     print(f"[{i+1}/{len(target_list)}] âš½ {title[:20]}...", end="")
    
#     img_url, details = get_club_details(title)
    
#     filename = f"{safe_id}.png"
#     save_path = os.path.join(IMG_DIR, filename)
#     local_ref = f"./assets/football_clubs_png/{filename}"
    
#     entry = {
#         "id": safe_id,
#         "name": title.split(" F.C.")[0].split(" CF")[0],
#         "league": item['league'],
#         "img": local_ref,
#         **details
#     }
    
#     # ä¸‹è½½å›¾ç‰‡
#     has_img = False
#     if img_url and not os.path.exists(save_path):
#         try:
#             content = requests.get(img_url, headers=HEADERS, proxies=PROXIES, verify=False, timeout=10).content
#             with open(save_path, 'wb') as f: f.write(content)
#             has_img = True
#             print(" ğŸ“¸", end="")
#         except: print(" âŒ", end="")
#     elif os.path.exists(save_path):
#         has_img = True
#         print(" â©", end="")
#     else:
#         print(" âš ï¸ æ— å›¾", end="")
        
#     print("") 
    
#     # åªæœ‰æŠ“åˆ°æ•°æ®æ‰å­˜ (é˜²æ­¢ç©ºæ¡ç›®)
#     existing_db.append(entry)
#     processed_ids.add(safe_id)
#     count_new += 1
    
#     if count_new % 10 == 0:
#         with open(JSON_FILE, 'w', encoding='utf-8') as f:
#             json.dump(existing_db, f, ensure_ascii=False, indent=2)

# with open(JSON_FILE, 'w', encoding='utf-8') as f:
#     json.dump(existing_db, f, ensure_ascii=False, indent=2)

# print(f"\nğŸ‰ ç¡¬æ ¸æŠ“å–å®Œæˆï¼å…±æ”¶å½• {len(existing_db)} æ”¯çƒé˜Ÿã€‚")
# print(f"ğŸ“‚ æ•°æ®ä½ç½®: {JSON_FILE}")
# # import json
# # import os

# # # --- âš™ï¸ é…ç½® ---
# # FILE_PATH = "./data/football_clubs_europe.json"

# # def remove_duplicates():
# #     if not os.path.exists(FILE_PATH):
# #         print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {FILE_PATH}")
# #         return

# #     print(f"ğŸ“‚ æ­£åœ¨è¯»å–: {FILE_PATH} ...")
    
# #     with open(FILE_PATH, 'r', encoding='utf-8') as f:
# #         try:
# #             data = json.load(f)
# #         except json.JSONDecodeError:
# #             print("âŒ JSON æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æ")
# #             return

# #     original_count = len(data)
# #     unique_data = []
# #     seen_names = set()
    
# #     # ç»Ÿè®¡é‡å¤æ¥æºï¼ˆå¯é€‰ï¼Œçœ‹çœ‹å“ªäº›è”èµ›é‡å¤äº†ï¼‰
# #     duplicates_found = []

# #     for item in data:
# #         # è·å–åå­—ï¼Œå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œç»Ÿä¸€è½¬å°å†™è¿›è¡Œæ¯”è¾ƒï¼ˆé˜²æ­¢ Arsenal å’Œ arsenal è¢«å½“æˆä¸¤ä¸ªï¼‰
# #         # ä½†ä¿å­˜æ—¶ä¿ç•™åŸæ ¼å¼
# #         raw_name = item.get('name', '').strip()
# #         compare_name = raw_name.lower()
        
# #         if not compare_name:
# #             continue

# #         if compare_name not in seen_names:
# #             unique_data.append(item)
# #             seen_names.add(compare_name)
# #         else:
# #             # å‘ç°é‡å¤
# #             duplicates_found.append(f"{raw_name} ({item.get('league', 'æœªçŸ¥')})")

# #     new_count = len(unique_data)
# #     removed_count = original_count - new_count

# #     # --- ä¿å­˜å›åŸæ–‡ä»¶ ---
# #     with open(FILE_PATH, 'w', encoding='utf-8') as f:
# #         json.dump(unique_data, f, ensure_ascii=False, indent=2)

# #     print("-" * 40)
# #     print(f"ğŸ‰ å»é‡å®Œæˆï¼")
# #     print(f"ğŸ”´ åŸæœ‰æ•°é‡: {original_count}")
# #     print(f"ğŸŸ¢ ç°æœ‰æ•°é‡: {new_count}")
# #     print(f"ğŸ—‘ï¸ ç§»é™¤é‡å¤: {removed_count} æ¡")
    
# #     if removed_count > 0:
# #         print("\nğŸ‘‡ è¢«ç§»é™¤çš„é‡å¤é¡¹ç¤ºä¾‹ (å‰5ä¸ª):")
# #         for d in duplicates_found[:5]:
# #             print(f"   - {d}")

# # if __name__ == "__main__":
# #     remove_duplicates()

import json
import os
from openai import OpenAI
import time

# --- âš™ï¸ é…ç½® ---
API_KEY = "sk-e30c53b48e4d4da1ae7055862bdade06" # ğŸ”¥ æ›¿æ¢ä½ çš„ DeepSeek Key
BASE_URL = "https://api.deepseek.com"

# æ—¢æ˜¯è¾“å…¥ä¹Ÿæ˜¯è¾“å‡ºï¼Œå®ç°â€œåŸåœ°åˆå¹¶â€
TARGET_FILE = "./data/football_clubs_hardcore.json"

client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

def translate_batch(names_list):
    """
    æ‰¹é‡ç¿»è¯‘å‡½æ•°
    """
    prompt = f"""
    è¯·å°†ä»¥ä¸‹è¶³çƒä¿±ä¹éƒ¨åç§°ç¿»è¯‘æˆ**ä¸­å›½å¤§é™†é€šç”¨ä¸­æ–‡è¯‘å**ã€‚
    
    è¾“å…¥åˆ—è¡¨ï¼š
    {json.dumps(names_list, ensure_ascii=False)}
    
    è¦æ±‚ï¼š
    1. è¿”å›ä¸€ä¸ªä¸¥æ ¼çš„ JSON å¯¹è±¡ï¼ŒKey æ˜¯è‹±æ–‡åŸåï¼ŒValue æ˜¯ä¸­æ–‡è¯‘åã€‚
    2. ç¤ºä¾‹ï¼š{{"Arsenal": "é˜¿æ£®çº³", "Inter Miami CF": "è¿ˆé˜¿å¯†å›½é™…"}}
    3. åªè¿”å› JSONï¼Œä¸è¦ Markdownã€‚
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "You are a football translator."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            response_format={ "type": "json_object" }
        )
        content = response.choices[0].message.content
        content = content.replace("```json", "").replace("```", "").strip()
        return json.loads(content)
    except Exception as e:
        print(f"âŒ API è°ƒç”¨å¤±è´¥: {e}")
        return {}

# --- ä¸»ç¨‹åº ---
if not os.path.exists(TARGET_FILE):
    print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {TARGET_FILE}")
    exit()

print(f"ğŸ“‚ è¯»å–æ•°æ®: {TARGET_FILE} ...")
with open(TARGET_FILE, 'r', encoding='utf-8') as f:
    clubs_data = json.load(f)

# ç»Ÿè®¡
total_clubs = len(clubs_data)
# ç­›é€‰å‡ºæ²¡æœ‰ name_zh å­—æ®µçš„çƒé˜Ÿ
needs_translation = [c for c in clubs_data]

print(f"ğŸ“Š æ€»çƒé˜Ÿæ•°: {total_clubs}")
print(f"ğŸ“ å¾…ç¿»è¯‘æ•°: {len(needs_translation)}")

if len(needs_translation) == 0:
    print("ğŸ‰ æ‰€æœ‰çƒé˜Ÿéƒ½å·²æœ‰ä¸­æ–‡åï¼Œæ— éœ€å¤„ç†ï¼")
    exit()

# åˆ†æ‰¹å¤„ç†
BATCH_SIZE = 30
current_batch_objs = [] # å­˜å¯¹è±¡å¼•ç”¨ï¼Œæ–¹ä¾¿ç›´æ¥ä¿®æ”¹
current_batch_names = [] # å­˜åå­—å‘ç»™API

processed_count = 0

print("ğŸš€ å¼€å§‹æ‰¹é‡ç¿»è¯‘å¹¶åˆå¹¶...")

for i, club in enumerate(needs_translation):
    current_batch_objs.append(club)
    current_batch_names.append(club['name'])
    
    # å½“æ‰¹æ¬¡æ»¡ï¼Œæˆ–è€…åˆ°è¾¾æœ€åä¸€ä¸ª
    if len(current_batch_names) >= BATCH_SIZE or i == len(needs_translation) - 1:
        
        print(f"ğŸ¤– æ­£åœ¨ç¿»è¯‘ {len(current_batch_names)} ä¸ªçƒé˜Ÿ...", end="")
        
        # è°ƒç”¨ API
        translations = translate_batch(current_batch_names)
        
        # ğŸ”¥ æ ¸å¿ƒæ­¥éª¤ï¼šç›´æ¥ä¿®æ”¹åŸå¯¹è±¡ï¼Œè¿½åŠ æ–°å­—æ®µ
        success_count = 0
        for obj in current_batch_objs:
            eng_name = obj['name']
            if eng_name in translations:
                # è¿™ä¸€æ­¥å°±æ˜¯â€œåˆå¹¶åˆ°åŸæœ¬çš„jsonä¸­ï¼Œæˆä¸ºæ–°çš„å­—æ®µâ€
                obj['name_zh'] = translations[eng_name]
                success_count += 1
            else:
                # å¦‚æœç¿»è¯‘å¤±è´¥ï¼Œæš‚æ—¶å¡«è‹±æ–‡ï¼Œé˜²æ­¢ä¸‹æ¬¡é‡å¤è¯·æ±‚
                obj['name_zh'] = eng_name
        
        print(f" âœ… æˆåŠŸå†™å…¥ {success_count} æ¡")
        
        # æ¸…ç©ºç¼“å†²åŒº
        current_batch_objs = []
        current_batch_names = []
        processed_count += success_count
        
        # å®æ—¶ä¿å­˜å›åŸæ–‡ä»¶ (è¦†ç›–å†™å…¥)
        if processed_count % (BATCH_SIZE * 2) == 0:
            with open(TARGET_FILE, 'w', encoding='utf-8') as f:
                json.dump(clubs_data, f, ensure_ascii=False, indent=2)
            print("ğŸ’¾ è¿›åº¦å·²ä¿å­˜")

# æœ€ç»ˆä¿å­˜
with open(TARGET_FILE, 'w', encoding='utf-8') as f:
    json.dump(clubs_data, f, ensure_ascii=False, indent=2)

print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼ä¸­æ–‡åå·²åˆå¹¶è‡³ {TARGET_FILE}")