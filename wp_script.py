import requests
from bs4 import BeautifulSoup
import json
import os
import re
import time
import urllib3

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- âš™ï¸ é…ç½® ---
# ğŸ”¥ğŸ”¥ğŸ”¥ è¯·åŠ¡å¿…ä¿®æ”¹ä¸ºä½ çš„ VPN ç«¯å£ (å¦‚ 7890, 1080) ğŸ”¥ğŸ”¥ğŸ”¥
PROXY_PORT = 7890 
PROXIES = {
    "http": f"http://127.0.0.1:{PROXY_PORT}",
    "https": f"http://127.0.0.1:{PROXY_PORT}"
}

INPUT_FILE = "./data/countries.json"
OUTPUT_FILE = "./data/countries_wiki_extra.json"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# --- ğŸ› ï¸ è¾…åŠ©å‡½æ•° ---
def clean_text(text):
    if not text: return "N/A"
    # å»é™¤å¼•ç”¨æ ‡ç­¾ [1][a]
    text = re.sub(r'\[.*?\]', '', text)
    # å»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def get_wiki_data(country_name, cca2):
    url = f"https://en.wikipedia.org/wiki/{country_name}"
    print(f"  ğŸŒ æ­£åœ¨æŠ“å–: {country_name} ({url})...")
    
    try:
        response = requests.get(url, headers=HEADERS, proxies=PROXIES, verify=False, timeout=15)
        if response.status_code != 200:
            print(f"  âŒ æ— æ³•è®¿é—®é¡µé¢ (Code: {response.status_code})")
            return None
            
        soup = BeautifulSoup(response.content, 'html.parser')
        infobox = soup.find('table', {'class': 'infobox'})
        
        if not infobox:
            print("  âš ï¸ æœªæ‰¾åˆ° Infobox")
            return None

        # å¾…æå–çš„å­—æ®µå­—å…¸
        data = {
            "id": cca2, # å…³è”é”®
            "capital": "N/A",
            "largest_city": "N/A",
            "official_languages": "N/A",
            "official_script": "N/A",
            "demonym": "N/A",
            "area_total": "N/A",
            "population_estimate": "N/A",
            "population_density": "N/A",
            "gdp_ppp_total": "N/A",
            "gdp_ppp_per_capita": "N/A",
            "gdp_nominal_total": "N/A",
            "gdp_nominal_per_capita": "N/A",
            "gini": "N/A",
            "currency": "N/A"
        }

        # éå†è¡¨æ ¼è¡Œ
        rows = infobox.find_all('tr')
        current_header = "" # ç”¨äºè®°å½•å½“å‰çš„äºŒçº§æ ‡é¢˜ï¼ˆå¦‚ Area, Populationï¼‰

        for tr in rows:
            th = tr.find('th')
            td = tr.find('td')
            
            # 1. æå–çº¯æ–‡æœ¬ç”¨äºåˆ¤æ–­
            header_text = clean_text(th.get_text()) if th else ""
            value_text = clean_text(td.get_text()) if td else ""
            
            # 2. å¤„ç†åˆ†èŠ‚æ ‡é¢˜ (å¦‚ --- Area ---)
            if th and not td: 
                # è¿™ç§é€šå¸¸æ˜¯åˆ†ç±»æ ‡é¢˜
                current_header = header_text.lower()
                continue

            if not th or not td: continue

            # 3. åŒ¹é…å­—æ®µ (ä½¿ç”¨å…³é”®è¯æ¨¡ç³ŠåŒ¹é…)
            # --- Capital ---
            if "capital" in header_text.lower():
                data["capital"] = value_text
            
            # --- Largest City ---
            elif "largest city" in header_text.lower():
                data["largest_city"] = value_text
            
            # --- Language ---
            elif "official language" in header_text.lower():
                data["official_languages"] = value_text
            
            # --- Script ---
            elif "official script" in header_text.lower():
                data["official_script"] = value_text

            # --- Demonym ---
            elif "demonym" in header_text.lower():
                data["demonym"] = value_text

            # --- Area ---
            elif "area" in current_header or "area" in header_text.lower():
                if "total" in header_text.lower() or data["area_total"] == "N/A":
                    data["area_total"] = value_text

            # --- Population ---
            elif "population" in current_header or "population" in header_text.lower():
                if "estimate" in header_text.lower() or "census" in header_text.lower():
                    data["population_estimate"] = value_text
                elif "density" in header_text.lower():
                    data["population_density"] = value_text

            # --- GDP (PPP) ---
            elif "gdp" in header_text.lower() and "ppp" in header_text.lower():
                current_header = "gdp_ppp" # æ ‡è®°è¿›å…¥ GDP PPP åŒºåŸŸ
            elif current_header == "gdp_ppp":
                if "total" in header_text.lower(): data["gdp_ppp_total"] = value_text
                elif "per capita" in header_text.lower(): data["gdp_ppp_per_capita"] = value_text

            # --- GDP (Nominal) ---
            elif "gdp" in header_text.lower() and "nominal" in header_text.lower():
                current_header = "gdp_nominal"
            elif current_header == "gdp_nominal":
                if "total" in header_text.lower(): data["gdp_nominal_total"] = value_text
                elif "per capita" in header_text.lower(): data["gdp_nominal_per_capita"] = value_text

            # --- Gini ---
            elif "gini" in header_text.lower():
                data["gini"] = value_text

            # --- Currency ---
            elif "currency" in header_text.lower():
                data["currency"] = value_text

        print(f"  âœ… è§£ææˆåŠŸ! (GDP: {data['gdp_nominal_total']})")
        return data

    except Exception as e:
        print(f"  âŒ å‘ç”Ÿé”™è¯¯: {e}")
        return None

# --- ä¸»ç¨‹åº ---
if __name__ == "__main__":
    if not os.path.exists(INPUT_FILE):
        print("âŒ è¯·å…ˆè¿è¡Œä¹‹å‰çš„è„šæœ¬ç”Ÿæˆ countries.json")
        exit()

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        countries = json.load(f)

    # å¦‚æœæœ‰æ—§çš„æ‰©å±•æ•°æ®ï¼Œè¯»å–å®ƒä»¥å…é‡å¤çˆ¬å– (æ–­ç‚¹ç»­ä¼ )
    wiki_db = {}
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            try:
                # è½¬æ¢æˆå­—å…¸æ–¹ä¾¿ç´¢å¼•: {'cn': {...}, 'us': {...}}
                existing_list = json.load(f)
                if isinstance(existing_list, dict): wiki_db = existing_list # å…¼å®¹æ—§æ ¼å¼
                else: wiki_db = {item['id']: item for item in existing_list}
            except: pass

    print(f"ğŸš€ å¼€å§‹çˆ¬å– {len(countries)} ä¸ªå›½å®¶çš„ Wiki æ‰©å±•ä¿¡æ¯...")
    
    count = 0
    for c in countries:
        cid = c['id']
        name_en = c.get('name_en', c.get('id').upper()) # ä¼˜å…ˆç”¨è‹±æ–‡åï¼Œæ²¡æœ‰åˆ™ç”¨ä»£ç 
        
        # ç‰¹æ®Šå¤„ç†ï¼šEChartsæ˜ å°„è¡¨é‡Œçš„åå­—å¯èƒ½å’ŒWikiä¸ä¸€è‡´ï¼Œä¼˜å…ˆç”¨ name_en (æˆ‘ä»¬åœ¨scraper_pro_v2é‡Œå·²ç»æ¸…æ´—è¿‡äº†)
        # å¦‚æœçˆ¬å–å¤±è´¥ï¼Œå¯ä»¥å°è¯•ç”¨ c['fullName']
        
        if cid in wiki_db and wiki_db[cid].get('capital') != "N/A":
            # print(f"â© è·³è¿‡ {name_en} (å·²å­˜åœ¨)")
            continue

        wiki_data = get_wiki_data(name_en, cid)
        
        if wiki_data:
            wiki_db[cid] = wiki_data
            count += 1
            
            # æ¯çˆ¬5ä¸ªä¿å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒç™½è·‘
            if count % 5 == 0:
                with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                    json.dump(wiki_db, f, ensure_ascii=False, indent=2)
                print("ğŸ’¾ ä¸´æ—¶ä¿å­˜æˆåŠŸ")
            
            # ç¤¼è²Œå»¶æ—¶ï¼Œé˜²å°IP
            # time.sleep(1) 

    # æœ€ç»ˆä¿å­˜ (ä¿å­˜ä¸ºå­—å…¸æ ¼å¼ï¼Œæ–¹ä¾¿å‰ç«¯é€šè¿‡ ID æŸ¥æ‰¾)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(wiki_db, f, ensure_ascii=False, indent=2)

    print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print(f"ğŸ“‚ æ‰©å±•æ•°æ®å·²ä¿å­˜è‡³: {OUTPUT_FILE}")
    print("ğŸ’¡ è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ JSON æ–‡ä»¶ï¼Œä½ å¯ä»¥é€šè¿‡ fetch('./data/countries_wiki_extra.json') æ¥åŠ è½½å®ƒã€‚")